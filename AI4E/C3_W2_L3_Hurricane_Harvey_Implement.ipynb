{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Satellite Imagery to detect disaster locations \n",
    "\n",
    "Now that you have explored the data and applied the CNN Models to predict whether a satellite image contains damage or not, you can use a visualization tool called, \"GradCAM\" to analyze which parts of a satellite image contribute to the binary predictions of your CNN Models, either damaged or undamaged. In addition, you will also explore geo-locations of identified damaged areas in an interactive map. \n",
    "\n",
    "In this lab, you will apply the following steps:\n",
    "\n",
    "1. Import Python packages\n",
    "2. Load the dataset\n",
    "3. Load the pre-trained model\n",
    "4. Evaluate Model\n",
    "5. Analyze classification features using GradCAM\n",
    "6. Explore geo-locations of identified damaged areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Import Python packages\n",
    "\n",
    "Run the next cell to import that Python packages you'll need for this lab.\n",
    "\n",
    "Note the `import utils` line. This line imports the functions that were specifically written for this lab. If you want to look at what these functions are, go to `File -> Open...` and open the `utils.py` file to have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "\n",
    "# Ignore tf warning messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = './data/test/'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dataset_dir, \n",
    "    target_size = (150, 150),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,                           \n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ7PSlyWNoaQ"
   },
   "source": [
    "You can define the correspondency between the model labels and the categories of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2cat = {1: 'visible_damage',\n",
    "             0: 'no_damage'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the pre-trained model\n",
    "\n",
    "Since the the model requires GPU and a lot of time to train, you will load the pre-trained model from the `models` folder. This is the same model that you have used in the design notebook.\n",
    "\n",
    "* `cnn_100epochs_Adam.hdf5`: Pre-trained CNN model using 100 epochs with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/cnn_100epochs_Adam.hdf5', compile=False)\n",
    "model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model\n",
    "\n",
    "Check the performance of the pre-trained model on the test set with the additional augmented images. The augmented dataset can create more variety in the dataset and simulate potential biases such as different camera angles, lightning conditions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_labels = test_generator.classes\n",
    "y_predictions = model.predict(test_generator)\n",
    "\n",
    "utils.display_confusion_matrix(y_labels, y_predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to remind that the model performance is very similar in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall = utils.get_performance_metrics(y_labels, y_predictions.reshape(-1))\n",
    "\n",
    "print(f'Accuracy:  {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall:    {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iuF4kh8N2oZ"
   },
   "source": [
    "## 5. Analyze classification features using GradCAM\n",
    "\n",
    "[GradCAM](https://arxiv.org/abs/1610.02391) is a visualization tool that can be applied to convolutional neural networks (CNNs) to highlight the important regions in an input image that contribute to the prediction made by the network. Since CNN models are often large and complex models, it is difficult to understand how the network is making its predictions. GradCAM creates a heatmap on the image where each pixel represents a weighted sum of the predicted class score.\n",
    "\n",
    "<img src='images/gradcam1.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Analyzing pairs of images with GradCAM\n",
    "\n",
    "To better illustrate the performance of the CNN, first analyze pairs of `damage` and `no_damage` images at the same location. Consider what features are highlighted in pink or blue by GradCAM. Did GradCAM make the correct interpretation or did it misinterpret features like shadows or blurriness in the image as evidence of damage? \n",
    "\n",
    "**Note:** The areas of \"high importance\" (pink) in the GradCAM images aren't attempting to show where there is evidence of damage or no damage, rather, these are just the areas of the images that carried the most weight in the determination of the model output. So, it's important to have a look and see what parts of the image are most important in making this determination and identify whether it looks like the model is focusing on the wrong things, or correctly identifying the important features in an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find \"damage\" and \"no_damage\" image pairs that have the same coordinates\n",
    "matches = utils.find_matching_images(test_dataset_dir)\n",
    "    \n",
    "utils.interact_with_slider(\n",
    "    utils.display_predictions_gradcam,\n",
    "    0, len(matches)-1,\n",
    "    model, label2cat, matches, test_dataset_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore geo-locations of identified damaged areas\n",
    "\n",
    "When you run the cell below, you'll be plotting up a map of the locations included in your dataset and your assessment of visible damage or no damage. At this point, it's also worth thinking about another sort of \"total damage\" assessment that you could provide with a tool like this. With an analysis of the total area or total number of structures or an estimate of the total number of people affected, you could start to put together an estimate of the total damage incurred with this storm. However, each of these estimates would come with different potential uncertainties and depending on the goals of your assessment you would need to weigh the tradeoffs between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about two minutes to run\n",
    "utils.show_predictions_on_map(\n",
    "    y_predictions,\n",
    "    test_generator.filenames,\n",
    "    {'green':(0.0, 0.4), 'orange':(0.4, 0.6), 'red':(0.6, 1)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red = \"Damage\"\n",
    "\n",
    "Orange = \"Low confidence (0.4 to 0.6) in either damage or not\"\n",
    "\n",
    "Green = \"No damage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
